# üìä An√°lisis y Pipeline ETL de Datos de E-commerce
Este proyecto presenta una soluci√≥n completa para el an√°lisis y procesamiento de un gran volumen de datos de eventos de un sitio de comercio electr√≥nico. Incluye un **An√°lisis Exploratorio de Datos (EDA)** para extraer insights iniciales y un **pipeline ETL** automatizado con **Airflow** y **Spark** que implementa una **arquitectura Medallion** (Bronze, Silver, Gold) para transformar los datos crudos en m√©tricas de negocio agregadas y listas para el an√°lisis.

## üß± Arquitectura del Proyecto
El pipeline de datos sigue la Arquitectura Medallion para asegurar la calidad, gobernanza y escalabilidad de los datos a trav√©s de diferentes capas de procesamiento.

1. **Capa Bronze (Datos Crudos)**
    - **Fuente:** Los datos crudos en formato CSV (2020-Apr.csv) se ingieren en esta capa sin ninguna modificaci√≥n.
    - **Prop√≥sito:** Almacenar los datos originales como una fuente de verdad hist√≥rica, permitiendo reprocesar el pipeline desde el inicio si es necesario.
2. **Capa Silver (Datos Limpios y Estructurados)**
    - **Proceso:** El job de Spark bronze_to_silver.py se encarga de la transformaci√≥n.
    - **Transformaciones Clave:**
        - **Lectura de datos:** Ingesta del fichero CSV.
        - **Limpieza:** Se eliminan filas con valores nulos en columnas cr√≠ticas como event_time y user_id.
        - **Normalizaci√≥n:** La columna event_time se convierte a un formato timestamp est√°ndar para facilitar los an√°lisis de series temporales.
    - **Formato de Salida:** Los datos limpios se almacenan en formato Parquet, que es altamente eficiente para consultas anal√≠ticas.
3. **Capa Gold (Datos Agregados y de Negocio)**
    - **Proceso:** El job de Spark silver_to_gold.py consume los datos de la capa Silver.
    - **Transformaciones Clave:**
        - **Filtrado**: Procesa los eventos correspondientes a una fecha espec√≠fica (processing_date).
        - **Agregaci√≥n:** Agrupa los datos por product_id, category_id y brand para calcular m√©tricas diarias de negocio.
        - **M√©tricas Calculadas:**
            - **Vistas totales (total_views).**
            - **A√±adidos al carrito (total_carts).**
            - **Compras totales (total_purchases).**
            - **Ingresos totales (total_revenue).**
            - **Conteo de usuarios √∫nicos (unique_users).**
    - **Formato de Salida:** Los datos agregados se guardan en formato Parquet y se particionan por fecha (processing_date) para optimizar las consultas.
## üîé An√°lisis Exploratorio de Datos (EDA)
El notebook notebooks/data_analysis.ipynb revela varios insights clave sobre el comportamiento de los usuarios en abril de 2020:

- **Distribuci√≥n de Eventos:** La gran mayor√≠a de los eventos son de tipo view (93.1%), seguido por cart (5.2%) y purchase (1.7%).
- **Marcas y Categor√≠as Populares:**
    - Las marcas Samsung y Apple dominan en t√©rminos de interacciones de los usuarios.
    - Las categor√≠as principales con m√°s eventos son electronics, appliances y computers.
- **Actividad por Hora:** La actividad de los usuarios alcanza su punto m√°ximo durante las horas de la tarde, entre las 12:00 y las 16:00 (UTC).
- **Funnel de Conversi√≥n:**
    - La tasa de conversi√≥n de vista a carrito es del 5.57%.
    - La tasa de conversi√≥n de carrito a compra es notablemente alta, con un 33.07%.
    - La tasa de conversi√≥n general (de vista a compra) es del 1.84%. Esto sugiere que el mayor desaf√≠o es incentivar a los usuarios para que a√±adan productos al carrito.

## üìÅ Estructura del Repositorio
```plaintext
.
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ data_analysis.ipynb      # EDA y an√°lisis de datos en profundidad.
‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pipeline_config.yaml   # (Opcional) Configuraci√≥n de rutas.
‚îÇ   ‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ecommerce_etl_dag.py   # Definici√≥n del DAG de Airflow.
‚îÇ   ‚îî‚îÄ‚îÄ spark_jobs/
‚îÇ       ‚îú‚îÄ‚îÄ bronze_to_silver.py  # Script de Spark para la capa Silver.
‚îÇ       ‚îî‚îÄ‚îÄ silver_to_gold.py    # Script de Spark para la capa Gold.
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ check_data.py            # Script para verificar los datos en las capas Silver y Gold.
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ pyproject.toml               # Dependencias y configuraci√≥n del proyecto.
```

## üö© Requisitos Previos
Python >= 3.11
Apache Airflow
Apache Spark
Una cuenta de Kaggle con el fichero kaggle.json configurado (para descargar el dataset).

## ‚öôÔ∏è Instalaci√≥n y Configuraci√≥n
1. Clonar el repositorio:
```sh
git clone <URL-DEL-REPOSITORIO>
cd bdsys-ec
```
2. Crear un entorno virtual e instalar dependencias:
> Se recomienda usar un entorno virtual para gestionar las dependencias del proyecto.

```sh
python -m venv .venv
source .venv/bin/activate
pip install -e .
```
Esto instalar√° todos los paquetes listados en pyproject.toml.

3. Configurar el dataset:
> El pipeline espera el dataset en el directorio data/. Como este directorio est√° excluido por .gitignore, debes crearlo y descargar los datos.
```sh
mkdir data
# Descarga el dataset de e-commerce de Kaggle
# (Aseg√∫rate de tener tu API token de Kaggle configurado)
kaggle datasets download -d mkechinov/ecommerce-events-history-in-cosmetics-shop -f 2020-Apr.csv -p data/
```

4. Configurar Airflow:
   
- Aseg√∫rate de que tu instancia de Airflow est√© en funcionamiento.
- Copia el DAG *pipelines/dags/ecommerce_etl_dag.py* a tu carpeta de DAGs de Airflow.
- En la UI de Airflow, configura las conexiones necesarias:
    - **fs_default:** Una conexi√≥n de tipo "File Path" que apunte al directorio ra√≠z del proyecto (/home/arturo/BDSyS-eC).
    - **spark_default:** Una conexi√≥n de tipo "Spark" que apunte a tu cl√∫ster de Spark.


## üöÄ C√≥mo Ejecutar el Pipeline
1. **Activa el DAG:** En la UI de Airflow, busca el DAG ecommerce_etl_pipeline_v1 y act√≠valo.
2. **Dispara el Pipeline:** Puedes disparar el DAG manualmente. Por defecto, procesar√° la fecha 2020-01-01, pero puedes modificarla en los par√°metros de configuraci√≥n al ejecutarlo ("processing_date": "YYYY-MM-DD").
3. **Monitorea la Ejecuci√≥n:** Sigue el progreso de las tareas en la vista de grafo de Airflow.
    - wait_for_input: Espera a que el fichero 2020-Apr.csv est√© disponible.
    - bronze_to_silver: Procesa los datos crudos a la capa Silver.
    - silver_to_gold: Procesa los datos de Silver a la capa Gold.

## ‚úÖ C√≥mo Verificar los Datos
Una vez que el pipeline se haya ejecutado correctamente, puedes usar el script de verificaci√≥n para inspeccionar los datos en las capas Silver y Gold.

```sh
spark-submit scripts/check_data.py
```

El script mostrar√° el conteo de filas, el esquema y las primeras 5 filas de cada tabla, confirmando que los datos se han procesado y almacenado correctamente.

## Pr√≥ximos Pasos
Este proyecto sienta las bases para an√°lisis m√°s avanzados. Los pr√≥ximos pasos sugeridos incluyen:

1. **An√°lisis de Cohortes:** Para entender el comportamiento del usuario a lo largo del tiempo y calcular el LTV.
2. **Market Basket Analysis:** Para identificar qu√© productos se compran juntos con frecuencia.
3. **Segmentaci√≥n de Clientes (RFM):** Para personalizar campa√±as de marketing.
4. **Sistemas de Recomendaci√≥n:** Para sugerir productos a los usuarios.
5. **An√°lisis de Elasticidad de Precios:** Para entender c√≥mo el precio afecta la probabilidad de compra.